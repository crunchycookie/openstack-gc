diff --git a/doc/source/admin/cpu-topologies.rst b/doc/source/admin/cpu-topologies.rst
index e1e92736de..082c88f655 100644
--- a/doc/source/admin/cpu-topologies.rst
+++ b/doc/source/admin/cpu-topologies.rst
@@ -772,11 +772,6 @@ while there could be other tools outside Nova to manage the governor, like
 tuned. That being said, we also provide a way to automatically change the
 governors on the fly, as explained below.
 
-.. important::
-   Some OS platforms don't support `cpufreq` resources in sysfs, so the
-   ``governor`` strategy could be not available. Please verify if your OS
-   supports scaling govenors before modifying the configuration option.
-
 If the strategy is set to ``governor``, a couple of config options are provided
 to define which exact CPU govenor to use for each of the up and down states :
 
diff --git a/nova/cmd/manage.py b/nova/cmd/manage.py
index 691945b80d..2e4026314f 100644
--- a/nova/cmd/manage.py
+++ b/nova/cmd/manage.py
@@ -3215,7 +3215,7 @@ class ImagePropertyCommands:
         'instance_uuid', metavar='<instance_uuid>',
         help='UUID of the instance')
     @args(
-        'image_property', metavar='<image_property>',
+        'property', metavar='<image_property>',
         help='Image property to show')
     def show(self, instance_uuid=None, image_property=None):
         """Show value of a given instance image property.
@@ -3233,10 +3233,10 @@ class ImagePropertyCommands:
             with context.target_cell(ctxt, im.cell_mapping) as cctxt:
                 instance = objects.Instance.get_by_uuid(
                     cctxt, instance_uuid, expected_attrs=['system_metadata'])
-                property_value = instance.system_metadata.get(
+                image_property = instance.system_metadata.get(
                     f'image_{image_property}')
-                if property_value:
-                    print(property_value)
+                if image_property:
+                    print(image_property)
                     return 0
                 else:
                     print(f'Image property {image_property} not found '
diff --git a/nova/cmd/status.py b/nova/cmd/status.py
index da4039d554..4a4e28d7e8 100644
--- a/nova/cmd/status.py
+++ b/nova/cmd/status.py
@@ -266,7 +266,7 @@ Instances found without hw_machine_type set. This warning can be ignored if
 your environment does not contain libvirt based compute hosts.
 Use the `nova-manage machine_type list_unset` command to list these instances.
 For more details see the following:
-https://docs.openstack.org/nova/latest/admin/hw-machine-type.html"""))
+https://docs.openstack.org/latest/nova/admin/hw_machine_type.html"""))
             return upgradecheck.Result(upgradecheck.Code.WARNING, msg)
 
         return upgradecheck.Result(upgradecheck.Code.SUCCESS)
@@ -276,7 +276,7 @@ https://docs.openstack.org/nova/latest/admin/hw-machine-type.html"""))
             msg = (_("""
 Service user token configuration is required for all Nova services.
 For more details see the following:
-https://docs.openstack.org/nova/latest/admin/configuration/service-user-token.html"""))  # noqa
+https://docs.openstack.org/latest/nova/admin/configuration/service-user-token.html"""))  # noqa
             return upgradecheck.Result(upgradecheck.Code.FAILURE, msg)
         return upgradecheck.Result(upgradecheck.Code.SUCCESS)
 
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index 8ca0e1498a..a09c9a9c3c 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -4205,7 +4205,8 @@ class ComputeManager(manager.Manager):
         nova_attachments = []
         bdms_to_delete = []
         for bdm in bdms.objects:
-            if bdm.volume_id and bdm.attachment_id:
+            if bdm.volume_id and bdm.source_type == 'volume' and \
+                bdm.destination_type == 'volume':
                 try:
                     self.volume_api.attachment_get(context, bdm.attachment_id)
                 except exception.VolumeAttachmentNotFound:
@@ -7011,9 +7012,9 @@ class ComputeManager(manager.Manager):
 
         instance.power_state = current_power_state
         # NOTE(mriedem): The vm_state has to be set before updating the
-        # resource tracker, see vm_states.allow_resource_removal(). The
-        # host/node values cannot be nulled out until after updating the
-        # resource tracker though.
+        # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
+        # values cannot be nulled out until after updating the resource tracker
+        # though.
         instance.vm_state = vm_states.SHELVED_OFFLOADED
         instance.task_state = None
         instance.save(expected_task_state=[task_states.SHELVING,
@@ -9188,16 +9189,12 @@ class ComputeManager(manager.Manager):
                                    objects.LibvirtVPMEMDevice)):
                         has_vpmem = True
                         break
-            power_management_possible = (
-                'dst_numa_info' in migrate_data and
-                migrate_data.dst_numa_info is not None)
             # No instance booting at source host, but instance dir
             # must be deleted for preparing next block migration
             # must be deleted for preparing next live migration w/o shared
             # storage
             # vpmem must be cleaned
-            do_cleanup = (not migrate_data.is_shared_instance_path or
-                          has_vpmem or power_management_possible)
+            do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
             destroy_disks = not migrate_data.is_shared_block_storage
         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
diff --git a/nova/compute/resource_tracker.py b/nova/compute/resource_tracker.py
index 9f92b98ba1..bcd3a671ae 100644
--- a/nova/compute/resource_tracker.py
+++ b/nova/compute/resource_tracker.py
@@ -1611,8 +1611,7 @@ class ResourceTracker(object):
         # NOTE(sfinucan): Both brand new instances as well as instances that
         # are being unshelved will have is_new_instance == True
         is_removed_instance = not is_new_instance and (is_removed or
-            vm_states.allow_resource_removal(
-                vm_state=instance['vm_state'], task_state=instance.task_state))
+            instance['vm_state'] in vm_states.ALLOW_RESOURCE_REMOVAL)
 
         if is_new_instance:
             self.tracked_instances.add(uuid)
@@ -1671,9 +1670,7 @@ class ResourceTracker(object):
 
         instance_by_uuid = {}
         for instance in instances:
-            if not vm_states.allow_resource_removal(
-                    vm_state=instance['vm_state'],
-                    task_state=instance.task_state):
+            if instance.vm_state not in vm_states.ALLOW_RESOURCE_REMOVAL:
                 self._update_usage_from_instance(context, instance, nodename)
             instance_by_uuid[instance.uuid] = instance
         return instance_by_uuid
diff --git a/nova/compute/stats.py b/nova/compute/stats.py
index e9180ec6d6..cfbee2e6bc 100644
--- a/nova/compute/stats.py
+++ b/nova/compute/stats.py
@@ -105,8 +105,7 @@ class Stats(dict):
         (vm_state, task_state, os_type, project_id) = \
                 self._extract_state_from_instance(instance)
 
-        if is_removed or vm_states.allow_resource_removal(
-                vm_state=vm_state, task_state=task_state):
+        if is_removed or vm_state in vm_states.ALLOW_RESOURCE_REMOVAL:
             self._decrement("num_instances")
             self.states.pop(uuid)
         else:
diff --git a/nova/compute/vm_states.py b/nova/compute/vm_states.py
index 393a3ebbf2..1a916ea59a 100644
--- a/nova/compute/vm_states.py
+++ b/nova/compute/vm_states.py
@@ -27,7 +27,6 @@ health and progress.
 See http://wiki.openstack.org/VMState
 """
 
-from nova.compute import task_states
 from nova.objects import fields
 
 
@@ -75,14 +74,8 @@ ALLOW_HARD_REBOOT = ALLOW_SOFT_REBOOT + [STOPPED, PAUSED, SUSPENDED, ERROR]
 # states we allow to trigger crash dump
 ALLOW_TRIGGER_CRASH_DUMP = [ACTIVE, PAUSED, RESCUED, RESIZED, ERROR]
 
+# states we allow resources to be freed in
+ALLOW_RESOURCE_REMOVAL = [DELETED, SHELVED_OFFLOADED]
+
 # states we allow for evacuate instance
 ALLOW_TARGET_STATES = [STOPPED]
-
-
-def allow_resource_removal(vm_state, task_state=None):
-    """(vm_state, task_state) combinations we allow resources to be freed in"""
-
-    return (
-        vm_state == DELETED or
-        vm_state == SHELVED_OFFLOADED and task_state != task_states.SPAWNING
-    )
diff --git a/nova/conf/compute.py b/nova/conf/compute.py
index de2743d850..9cf7b3499c 100644
--- a/nova/conf/compute.py
+++ b/nova/conf/compute.py
@@ -878,7 +878,56 @@ Related options:
   where ``VCPU`` resources should be allocated from.
 * ``vcpu_pin_set``: A legacy option that this option partially replaces.
 """),
-    cfg.BoolOpt('live_migration_wait_for_vif_plug',
+cfg.StrOpt('cpu_dynamic_set',
+        help="""
+Mask of host CPUs that can undergone dynamic performance changes, such as being strictly low powered.
+
+Possible values:
+
+* A comma-separated list of physical CPU numbers that instance VCPUs can be
+  allocated from. Each element should be either a single CPU number, a range of
+  CPU numbers, or a caret followed by a CPU number to be excluded from a
+  previous range. For example::
+
+    cpu_dedicated_set = "4-12,^8,15"
+
+Related options:
+
+* ``[compute] cpu_stable_set``: This is the parent option for defining the superset
+  where ``VCPU`` resources should be allocated from. ``[compute] cpu_dynamic_set`` option complements 
+  this option by enabling physical CPU prioritization.
+"""),
+   cfg.StrOpt('cpu_stable_set',
+        help="""
+Mask of host CPUs that can be used for prioritized ``PCPU`` resources.
+
+Possible values:
+
+* A comma-separated list of physical CPU numbers that instance VCPUs can be
+  allocated from. Each element should be either a single CPU number, a range of
+  CPU numbers, or a caret followed by a CPU number to be excluded from a
+  previous range. For example::
+
+    cpu_dedicated_set = "4-12,^8,15"
+
+Related options:
+
+* ``[compute] cpu_dedicated_set``: This is the parent option for defining the superset
+  where ``VCPU`` resources should be allocated from. ``[compute] cpu_stable_set`` option complements 
+  this option by enabling physical CPU prioritization.
+"""),
+   cfg.StrOpt('cpu_sleep_info_endpoint',
+        help="""
+An endpoint which provides information cpu ids that are at the sleep mode. Such cpus are then considered offline in 
+openstack.
+
+Possible values:
+
+* A url to which this node can make a HTTP GET request without authentication (a security established resource). For example::
+
+    cpu_sleep_info_endpoint = "http://localhost:3000/gc-controller/is-asleep"
+"""),
+   cfg.BoolOpt('live_migration_wait_for_vif_plug',
         default=True,
         help="""
 Determine if the source compute host should wait for a ``network-vif-plugged``
diff --git a/nova/network/neutron.py b/nova/network/neutron.py
index f37f8a343a..e6d47c9a6b 100644
--- a/nova/network/neutron.py
+++ b/nova/network/neutron.py
@@ -3610,7 +3610,6 @@ class API:
                            'gateway': network_model.IP(
                                 address=subnet['gateway_ip'],
                                 type='gateway'),
-                           'enable_dhcp': False,
             }
             if subnet.get('ipv6_address_mode'):
                 subnet_dict['ipv6_address_mode'] = subnet['ipv6_address_mode']
@@ -3627,14 +3626,22 @@ class API:
                         subnet_dict['dhcp_server'] = ip_pair['ip_address']
                         break
 
-            # NOTE(stblatzheim): If enable_dhcp is set on subnet, but subnet
-            # has ovn native dhcp and no dhcp-agents. Network owner will be
-            # network:distributed
-            # Just rely on enable_dhcp flag given by neutron
-            # Fix for https://bugs.launchpad.net/nova/+bug/2055245
-
-            if subnet.get('enable_dhcp'):
-                subnet_dict['enable_dhcp'] = True
+            # NOTE(arnaudmorin): If enable_dhcp is set on subnet, but, for
+            # some reason neutron did not have any DHCP port yet, we still
+            # want the network_info to be populated with a valid dhcp_server
+            # value. This is mostly useful for the metadata API (which is
+            # relying on this value to give network_data to the instance).
+            #
+            # This will also help some providers which are using external
+            # DHCP servers not handled by neutron.
+            # In this case, neutron will never create any DHCP port in the
+            # subnet.
+            #
+            # Also note that we cannot set the value to None because then the
+            # value would be discarded by the metadata API.
+            # So the subnet gateway will be used as fallback.
+            if subnet.get('enable_dhcp') and 'dhcp_server' not in subnet_dict:
+                subnet_dict['dhcp_server'] = subnet['gateway_ip']
 
             subnet_object = network_model.Subnet(**subnet_dict)
             for dns in subnet.get('dns_nameservers', []):
diff --git a/nova/objects/instance_numa.py b/nova/objects/instance_numa.py
index b7d69a90f9..28e877574d 100644
--- a/nova/objects/instance_numa.py
+++ b/nova/objects/instance_numa.py
@@ -293,12 +293,6 @@ class InstanceNUMATopology(base.NovaObject,
             cell.cpu_pinning.values() for cell in self.cells
             if cell.cpu_pinning]))
 
-    @property
-    def cpuset_reserved(self):
-        return set(itertools.chain.from_iterable([
-            cell.cpuset_reserved for cell in self.cells
-            if cell.cpuset_reserved]))
-
     def clear_host_pinning(self):
         """Clear any data related to how instance is pinned to the host.
 
diff --git a/nova/objects/numa.py b/nova/objects/numa.py
index 36f51201b0..d443581221 100644
--- a/nova/objects/numa.py
+++ b/nova/objects/numa.py
@@ -98,6 +98,14 @@ class NUMACell(base.NovaObject):
         self.pinned_cpus |= cpus
 
     def unpin_cpus(self, cpus):
+        # if the core is offline, below check can fail, although there are cpus that are not a part of available cpus.
+        # For openstack-gc, we disable this check, assuming its a core that went to sleep. When prepping for production,
+        # an additional check to verify that state (call external endpoint to see if the cpu is actually green and went
+        # asleep) can be included.
+        if (cpus - self.pcpuset) and ((self.pinned_cpus & cpus) != cpus):
+            self.pinned_cpus -= cpus
+            return
+
         if cpus - self.pcpuset:
             raise exception.CPUUnpinningUnknown(requested=list(cpus),
                                                 available=list(self.pcpuset))
diff --git a/nova/scheduler/filters/compute_filter.py b/nova/scheduler/filters/compute_filter.py
index 21c7fd4d3d..2da3648dd5 100644
--- a/nova/scheduler/filters/compute_filter.py
+++ b/nova/scheduler/filters/compute_filter.py
@@ -17,6 +17,7 @@ from oslo_log import log as logging
 
 from nova.scheduler import filters
 from nova import servicegroup
+from ..manager import CORE_USAGE
 
 LOG = logging.getLogger(__name__)
 
@@ -34,6 +35,7 @@ class ComputeFilter(filters.BaseHostFilter):
 
     def host_passes(self, host_state, spec_obj):
         """Returns True for only active compute nodes."""
+
         service = host_state.service
         if service['disabled']:
             LOG.debug("%(host_state)s is disabled, reason: %(reason)s",
@@ -45,4 +47,18 @@ class ComputeFilter(filters.BaseHostFilter):
                 LOG.warning("%(host_state)s has not been heard from in a "
                             "while", {'host_state': host_state})
                 return False
+
+        host_ip = host_state.host_ip
+        core_usage = list(filter(lambda x: x['host-ip'] == str(host_ip), CORE_USAGE['core_usage']))
+        core_usage = core_usage[0]
+
+        rcpus_avl = core_usage['reg-cores-avl']
+        rcpus_usg = core_usage['reg-cores-usg']
+        rcpus_free = rcpus_avl - rcpus_usg
+
+        hints = spec_obj.scheduler_hints
+        type = hints['type'][0]
+        if type == 'regular' and rcpus_free < spec_obj.vcpus:
+            return False
+
         return True
diff --git a/nova/scheduler/manager.py b/nova/scheduler/manager.py
index 620519d403..4c0e05f111 100644
--- a/nova/scheduler/manager.py
+++ b/nova/scheduler/manager.py
@@ -23,6 +23,7 @@ import collections
 import copy
 import random
 
+import requests
 from keystoneauth1 import exceptions as ks_exc
 from oslo_log import log as logging
 import oslo_messaging as messaging
@@ -52,6 +53,8 @@ QUOTAS = quota.QUOTAS
 
 HOST_MAPPING_EXISTS_WARNING = False
 
+CORE_USAGE = {}
+
 
 class SchedulerManager(manager.Manager):
     """Chooses a host to run instances on.
@@ -702,6 +705,11 @@ class SchedulerManager(manager.Manager):
         scheduling constraints for the request spec object and have been sorted
         according to the weighers.
         """
+        # todo following url needs to be read from the configuration file.
+        # gc-emulation service tracks green core usage across nodes.
+        core_usages = requests.get(url='http://{GC_EMULATION_SERVICE_HOST}:{GC_EMULATION_SERVICE_PORT}/gc/core-usage').json()
+        global CORE_USAGE
+        CORE_USAGE['core_usage'] = core_usages
         filtered_hosts = self.host_manager.get_filtered_hosts(host_states,
             spec_obj, index)
 
diff --git a/nova/scheduler/weights/cpu.py b/nova/scheduler/weights/cpu.py
index 904a788b46..fd26502be0 100644
--- a/nova/scheduler/weights/cpu.py
+++ b/nova/scheduler/weights/cpu.py
@@ -21,14 +21,111 @@ stacking, you can set the 'cpu_weight_multiplier' option (by configuration
 or aggregate metadata) to a negative number and the weighing has the opposite
 effect of the default.
 """
+import math
 
 import nova.conf
 from nova.scheduler import utils
 from nova.scheduler import weights
+from nova.weights import LOG
+from ..manager import CORE_USAGE
 
 CONF = nova.conf.CONF
 
 
+def get_prefer_non_empty_machines_score(usable_cores, used_cores):
+    free_cores = usable_cores - used_cores
+    if free_cores == 0:
+        return 0
+    return 1
+
+
+def get_prefer_most_unused_green_cores_score(gcpus_avl, gcpus_used):
+    MAX_CPUS = 16.0
+    free_gcpus = gcpus_avl - gcpus_used
+    score = free_gcpus / MAX_CPUS
+    return score
+
+
+def get_prefer_guranteed_renewable_draw_score(type, rcpus_avl, rcpus_used, gcpus_avl, gcpus_used, vm_cpus):
+    if type == 'regular':
+        return 0
+
+    rcpus_free = rcpus_avl - rcpus_used
+    gcpus_free = gcpus_avl - gcpus_used
+    rcpus_overflow = vm_cpus - rcpus_free
+
+    if 0 < rcpus_overflow <= gcpus_free:
+        return 1
+
+    return 0
+
+
+def get_worst_fit_on_green_cores_score(rcpus_avl, rcpus_used, gcpus_avl, gcpus_used, usable_cores, used_cores, vm_cpus):
+    rcpus_free = rcpus_avl - rcpus_used
+    gcpus_free = gcpus_avl - gcpus_used
+    is_alloc_on_gcpus = rcpus_free < vm_cpus <= (rcpus_free + gcpus_free)
+    if not is_alloc_on_gcpus:
+        return 0
+
+    score = 1 - get_best_fit_score(usable_cores=usable_cores, used_cores=used_cores, vm_cpus=vm_cpus)
+    return score
+
+
+def get_best_fit_score(usable_cores, used_cores, vm_cpus):
+    free_cores = usable_cores - used_cores
+    raw_score = free_cores - vm_cpus
+    score = 1 - raw_score / usable_cores
+    return score
+
+
+def get_cpu_attrs(host_state):
+    vcpus_used = host_state.vcpus_used
+    vcpus_free = (host_state.vcpus_total * 1.0 - host_state.vcpus_used)
+    rcpus_used = host_state.rcpus_used
+    rcpus_free = (host_state.rcpus_total * 1.0 - host_state.rcpus_used)
+    gcpus_used = host_state.gcpus_used
+    gcpus_free = (host_state.gcpus_total * 1.0 - host_state.gcpus_used)
+    return gcpus_free, gcpus_used, rcpus_free, rcpus_used, vcpus_free, vcpus_used
+
+
+def get_final_weight(usable_cores, used_cores, gcpus_avl, gcpus_used, rcpus_avl, rcpus_used, type, vm_cpus):
+
+    w1 = math.pow(3, 4) * get_prefer_non_empty_machines_score(
+        usable_cores=usable_cores,
+        used_cores=used_cores
+    )
+    w2 = math.pow(3, 3) * get_prefer_most_unused_green_cores_score(
+        gcpus_avl=gcpus_avl,
+        gcpus_used=gcpus_used
+    )
+    w3 = math.pow(3, 2) * get_prefer_guranteed_renewable_draw_score(
+        type=type,
+        rcpus_used=rcpus_used,
+        gcpus_used=gcpus_used,
+        gcpus_avl=gcpus_avl,
+        rcpus_avl=rcpus_avl,
+        vm_cpus=vm_cpus
+    )
+    w4 = math.pow(3, 1) * get_worst_fit_on_green_cores_score(
+        usable_cores=usable_cores,
+        used_cores=used_cores,
+        gcpus_avl=gcpus_avl,
+        gcpus_used=gcpus_used,
+        rcpus_avl=rcpus_avl,
+        rcpus_used=rcpus_used,
+        vm_cpus=vm_cpus
+    )
+    w5 = math.pow(3, 0) * get_best_fit_score(
+        usable_cores=usable_cores,
+        used_cores=used_cores,
+        vm_cpus=vm_cpus
+    )
+
+    final_weight = w1 + w2 + w3 + w4 + w5
+
+    return final_weight
+
+
 class CPUWeigher(weights.BaseHostWeigher):
     minval = 0
 
@@ -40,7 +137,31 @@ class CPUWeigher(weights.BaseHostWeigher):
 
     def _weigh_object(self, host_state, weight_properties):
         """Higher weights win.  We want spreading to be the default."""
-        vcpus_free = (
-            host_state.vcpus_total * host_state.cpu_allocation_ratio -
-            host_state.vcpus_used)
-        return vcpus_free
+        host_ip = host_state.host_ip
+        core_usage = list(filter(lambda x: x['host-ip'] == str(host_ip), CORE_USAGE['core_usage']))
+        core_usage = core_usage[0]
+
+        rcpus_avl = core_usage['reg-cores-avl']
+        gcpus_avl = core_usage['green-cores-avl']
+        rcpus_used = core_usage['reg-cores-usg']
+        gcpus_used = core_usage['green-cores-usg']
+
+        usable_cores = rcpus_avl + gcpus_avl
+        used_cores = rcpus_used + gcpus_used
+
+        hints = weight_properties.scheduler_hints
+        vm_type = hints['type'][0]
+
+        vm_cpus = weight_properties.vcpus
+
+        final_weight = get_final_weight(
+            usable_cores=usable_cores,
+            used_cores=used_cores,
+            gcpus_avl=gcpus_avl,
+            gcpus_used=gcpus_used,
+            rcpus_avl=rcpus_avl,
+            rcpus_used=rcpus_used,
+            type=vm_type,
+            vm_cpus=vm_cpus
+        )
+        return final_weight
diff --git a/nova/tests/fixtures/cinder.py b/nova/tests/fixtures/cinder.py
index 9ee0169d90..d4a0b36e49 100644
--- a/nova/tests/fixtures/cinder.py
+++ b/nova/tests/fixtures/cinder.py
@@ -101,9 +101,7 @@ class CinderFixture(fixtures.Fixture):
         # multi-attach, as some flows create a blank 'reservation' attachment
         # before deleting another attachment. However, a non-multiattach volume
         # can only have at most one attachment with a host connector at a time.
-        self.volumes = collections.defaultdict(dict)
         self.volume_to_attachment = collections.defaultdict(dict)
-        self.volume_snapshots = collections.defaultdict(dict)
 
     def setUp(self):
         super().setUp()
@@ -167,15 +165,6 @@ class CinderFixture(fixtures.Fixture):
         self.useFixture(fixtures.MockPatch(
             'nova.volume.cinder.API.attachment_get_all',
             side_effect=self.fake_attachment_get_all, autospec=False))
-        self.useFixture(fixtures.MockPatch(
-            'nova.volume.cinder.API.create_snapshot_force',
-            side_effect=self.fake_create_snapshot_force, autospec=False))
-        self.useFixture(fixtures.MockPatch(
-            'nova.volume.cinder.API.get_snapshot',
-            side_effect=self.fake_get_snapshot, autospec=False))
-        self.useFixture(fixtures.MockPatch(
-            'nova.volume.cinder.API.create',
-            side_effect=self.fake_vol_create, autospec=False))
 
     def _is_multiattach(self, volume_id):
         return volume_id in [
@@ -229,16 +218,13 @@ class CinderFixture(fixtures.Fixture):
         return {'save_volume_id': new_volume_id}
 
     def fake_get(self, context, volume_id, microversion=None):
-        if volume_id in self.volumes:
-            volume = self.volumes[volume_id]
-        else:
-            volume = {
-                'display_name': volume_id,
-                'id': volume_id,
-                'size': 1,
-                'multiattach': self._is_multiattach(volume_id),
-                'availability_zone': self.az
-            }
+        volume = {
+            'display_name': volume_id,
+            'id': volume_id,
+            'size': 1,
+            'multiattach': self._is_multiattach(volume_id),
+            'availability_zone': self.az
+        }
 
         # Add any attachment details the fixture has
         fixture_attachments = self.volume_to_attachment[volume_id]
@@ -480,36 +466,3 @@ class CinderFixture(fixtures.Fixture):
 
     def delete_vol_attachment(self, vol_id):
         del self.volume_to_attachment[vol_id]
-
-    def fake_create_snapshot_force(self, _ctxt, volume_id, name, description):
-        _id = uuidutils.generate_uuid()
-        snapshot = {
-            'id': _id,
-            'volume_id': volume_id,
-            'display_name': name,
-            'display_description': description,
-            'status': 'creating',
-            }
-        self.volume_snapshots[_id] = snapshot
-        return snapshot
-
-    def fake_get_snapshot(self, _ctxt, snap_id):
-        if snap_id in self.volume_snapshots:
-            # because instance is getting unquiesce_instance
-            self.volume_snapshots[snap_id]['status'] = 'available'
-            return self.volume_snapshots[snap_id]
-
-    def fake_vol_create(self, _ctxt, size, name, description,
-            snapshot=None, image_id=None, volume_type=None, metadata=None,
-            availability_zone=None):
-        _id = uuidutils.generate_uuid()
-        volume = {
-            'id': _id,
-            'status': 'available',
-            'display_name': name or 'fake-cinder-vol',
-            'attach_status': 'detached',
-            'size': size,
-            'display_description': description or 'fake-description',
-        }
-        self.volumes[_id] = volume
-        return volume
diff --git a/nova/tests/fixtures/filesystem.py b/nova/tests/fixtures/filesystem.py
index 0193e6d745..932d42fe27 100644
--- a/nova/tests/fixtures/filesystem.py
+++ b/nova/tests/fixtures/filesystem.py
@@ -39,14 +39,10 @@ class TempFileSystemFixture(fixtures.Fixture):
 
 
 class SysFileSystemFixture(TempFileSystemFixture):
-    def __init__(self, cpus_supported=None, cpufreq_enabled=True):
-        """Instantiates a fake sysfs.
+    """Creates a fake /sys filesystem"""
 
-        :param cpus_supported: number of devices/system/cpu (default: 10)
-        :param cpufreq_enabled: cpufreq subdir created (default: True)
-        """
+    def __init__(self, cpus_supported=None):
         self.cpus_supported = cpus_supported or 10
-        self.cpufreq_enabled = cpufreq_enabled
 
     def _setUp(self):
         super()._setUp()
@@ -77,12 +73,9 @@ class SysFileSystemFixture(TempFileSystemFixture):
 
         for cpu_nr in range(self.cpus_supported):
             cpu_dir = os.path.join(self.cpu_path_mock % {'core': cpu_nr})
-            os.makedirs(cpu_dir)
-            filesystem.write_sys(os.path.join(cpu_dir, 'online'), data='1')
-            if self.cpufreq_enabled:
-                os.makedirs(os.path.join(cpu_dir, 'cpufreq'))
-                filesystem.write_sys(
-                    os.path.join(cpu_dir, 'cpufreq/scaling_governor'),
-                    data='powersave')
+            os.makedirs(os.path.join(cpu_dir, 'cpufreq'))
+            filesystem.write_sys(
+                os.path.join(cpu_dir, 'cpufreq/scaling_governor'),
+                data='powersave')
         filesystem.write_sys(core.AVAILABLE_PATH,
                              f'0-{self.cpus_supported - 1}')
diff --git a/nova/tests/fixtures/glance.py b/nova/tests/fixtures/glance.py
index b1124036d7..b718f28c2a 100644
--- a/nova/tests/fixtures/glance.py
+++ b/nova/tests/fixtures/glance.py
@@ -300,10 +300,6 @@ class GlanceFixture(fixtures.Fixture):
 
         image_meta = copy.deepcopy(metadata)
 
-        if image_meta.get('min_disk'):
-            # min_disk should be of int type only.
-            image_meta['min_disk'] = int(image_meta['min_disk'])
-
         # Glance sets the size value when an image is created, so we
         # need to do that here to fake things out if it's not provided
         # by the caller. This is needed to avoid a KeyError in the
diff --git a/nova/tests/functional/integrated_helpers.py b/nova/tests/functional/integrated_helpers.py
index 6cb4172924..0d592fbdbe 100644
--- a/nova/tests/functional/integrated_helpers.py
+++ b/nova/tests/functional/integrated_helpers.py
@@ -18,7 +18,6 @@ Provides common functionality for integrated unit tests
 """
 
 import collections
-import datetime
 import random
 import re
 import string
@@ -27,7 +26,6 @@ import time
 from oslo_concurrency import lockutils
 from oslo_log import log as logging
 import oslo_messaging as messaging
-from oslo_utils.fixture import uuidsentinel as uuids
 
 from nova.compute import instance_actions
 from nova.compute import rpcapi as compute_rpcapi
@@ -670,42 +668,6 @@ class InstanceHelperMixin:
             self.cinder.create_vol_attachment(
                 volume_id, server['id'])
 
-    def _create_server_boot_from_volume(self):
-        bfv_image_id = uuids.bfv_image_uuid
-        timestamp = datetime.datetime(2011, 1, 1, 1, 2, 3)
-
-        image = {
-            'id': bfv_image_id,
-            'name': 'fake_image_name',
-            'created_at': timestamp,
-            'updated_at': timestamp,
-            'deleted_at': None,
-            'deleted': False,
-            'status': 'active',
-            'container_format': 'raw',
-            'disk_format': 'raw',
-            'min_disk': 0
-        }
-
-        self.glance.create(None, image)
-
-        # for bfv, image is not required in server request
-        server = self._build_server()
-        server.pop('imageRef')
-
-        # as bfv-image will be used as source in block_device_mapping_v2
-        # here block device will be created based on bfv-image
-        # i.e bfvimage_id
-        server['block_device_mapping_v2'] = [{
-            'source_type': 'image',
-            'destination_type': 'volume',
-            'boot_index': 0,
-            'uuid': bfv_image_id,
-            'volume_size': 1,
-        }]
-        server = self.api.post_server({'server': server})
-        return self._wait_for_state_change(server, 'ACTIVE')
-
 
 class PlacementHelperMixin:
     """A helper mixin for interacting with placement."""
diff --git a/nova/tests/functional/libvirt/test_power_manage.py b/nova/tests/functional/libvirt/test_power_manage.py
index f6e4b1ba5b..dc417712cd 100644
--- a/nova/tests/functional/libvirt/test_power_manage.py
+++ b/nova/tests/functional/libvirt/test_power_manage.py
@@ -59,15 +59,8 @@ class PowerManagementTestsBase(base.ServersTestBase):
             'hw:cpu_policy': 'dedicated',
             'hw:cpu_thread_policy': 'prefer',
         }
-        self.isolate_extra_spec = {
-            'hw:cpu_policy': 'dedicated',
-            'hw:cpu_thread_policy': 'prefer',
-            'hw:emulator_threads_policy': 'isolate',
-        }
         self.pcpu_flavor_id = self._create_flavor(
             vcpu=4, extra_spec=self.extra_spec)
-        self.isolate_flavor_id = self._create_flavor(
-            vcpu=4, extra_spec=self.isolate_extra_spec)
 
     def _assert_server_cpus_state(self, server, expected='online'):
         inst = objects.Instance.get_by_uuid(self.ctxt, server['id'])
@@ -91,136 +84,6 @@ class PowerManagementTestsBase(base.ServersTestBase):
                 self.assertEqual('performance', core.governor)
 
 
-class FakeCore(object):
-
-    def __init__(self, i):
-        self.ident = i
-        self.power_state = 'online'
-
-    @property
-    def online(self):
-        return self.power_state == 'online'
-
-    @online.setter
-    def online(self, state):
-        if state:
-            self.power_state = 'online'
-        else:
-            self.power_state = 'offline'
-
-
-class CoresStub(object):
-
-    def __init__(self):
-        self.cores = {}
-
-    def __call__(self, i):
-        if i not in self.cores:
-            self.cores[i] = FakeCore(i)
-        return self.cores[i]
-
-
-class PowerManagementLiveMigrationTestsBase(base.LibvirtMigrationMixin,
-                                            PowerManagementTestsBase):
-
-    def setUp(self):
-        super().setUp()
-
-        self.useFixture(nova_fixtures.SysFileSystemFixture())
-        self.flags(cpu_dedicated_set='1-9', cpu_shared_set=None,
-                   group='compute')
-        self.flags(vcpu_pin_set=None)
-        self.flags(cpu_power_management=True, group='libvirt')
-
-        # NOTE(artom) Fill up all dedicated CPUs (either with only the
-        # instance's CPUs, or instance CPUs + 1 emulator thread). This makes
-        # the assertions further down easier.
-        self.pcpu_flavor_id = self._create_flavor(
-            vcpu=9, extra_spec=self.extra_spec)
-        self.isolate_flavor_id = self._create_flavor(
-            vcpu=8, extra_spec=self.isolate_extra_spec)
-
-        self.start_compute(
-            host_info=fakelibvirt.HostInfo(cpu_nodes=1, cpu_sockets=1,
-                                           cpu_cores=5, cpu_threads=2),
-            hostname='src')
-        self.src = self.computes['src']
-        self.src.driver.cpu_api.core = CoresStub()
-        # NOTE(artom) In init_host() the libvirt driver calls
-        # power_down_all_dedicated_cpus(). Call it again now after swapping to
-        # our stub to fake reality.
-        self.src.driver.cpu_api.power_down_all_dedicated_cpus()
-
-        self.start_compute(
-            host_info=fakelibvirt.HostInfo(cpu_nodes=1, cpu_sockets=1,
-                                           cpu_cores=5, cpu_threads=2),
-            hostname='dest')
-        self.dest = self.computes['dest']
-        self.dest.driver.cpu_api.power_down_all_dedicated_cpus()
-
-    def assert_cores(self, host, cores, online=True):
-        for i in cores:
-            self.assertEqual(online, host.driver.cpu_api.core(i).online)
-
-
-class PowerManagementLiveMigrationTests(PowerManagementLiveMigrationTestsBase):
-
-    def test_live_migrate_server(self):
-        self.server = self._create_server(
-            flavor_id=self.pcpu_flavor_id,
-            expected_state='ACTIVE', host='src')
-        server = self._live_migrate(self.server)
-        self.assertEqual('dest', server['OS-EXT-SRV-ATTR:host'])
-        # We've powered down the source cores, and powered up the destination
-        # ones.
-        self.assert_cores(self.src, range(1, 10), online=False)
-        self.assert_cores(self.dest, range(1, 10), online=True)
-
-    def test_live_migrate_server_with_emulator_threads_isolate(self):
-        self.server = self._create_server(
-            flavor_id=self.isolate_flavor_id,
-            expected_state='ACTIVE', host='src')
-        server = self._live_migrate(self.server)
-        self.assertEqual('dest', server['OS-EXT-SRV-ATTR:host'])
-        # We're using a flavor with 8 CPUs, but with the extra dedicated CPU
-        # for the emulator threads, we expect all 9 cores to be powered up on
-        # the dest, and down on the source.
-        self.assert_cores(self.src, range(1, 10), online=False)
-        self.assert_cores(self.dest, range(1, 10), online=True)
-
-
-class PowerManagementLiveMigrationRollbackTests(
-    PowerManagementLiveMigrationTestsBase):
-
-    def _migrate_stub(self, domain, destination, params, flags):
-        conn = self.src.driver._host.get_connection()
-        dom = conn.lookupByUUIDString(self.server['id'])
-        dom.fail_job()
-
-    def test_live_migrate_server_rollback(self):
-        self.server = self._create_server(
-            flavor_id=self.pcpu_flavor_id,
-            expected_state='ACTIVE', host='src')
-        server = self._live_migrate(self.server,
-                                    migration_expected_state='failed')
-        self.assertEqual('src', server['OS-EXT-SRV-ATTR:host'])
-        self.assert_cores(self.src, range(1, 10), online=True)
-        self.assert_cores(self.dest, range(1, 10), online=False)
-
-    def test_live_migrate_server_with_emulator_threads_isolate_rollback(self):
-        self.server = self._create_server(
-            flavor_id=self.isolate_flavor_id,
-            expected_state='ACTIVE', host='src')
-        server = self._live_migrate(self.server,
-                                    migration_expected_state='failed')
-        self.assertEqual('src', server['OS-EXT-SRV-ATTR:host'])
-        # We're using a flavor with 8 CPUs, but with the extra dedicated CPU
-        # for the emulator threads, we expect all 9 cores to be powered back
-        # down on the dest, and up on the source.
-        self.assert_cores(self.src, range(1, 10), online=True)
-        self.assert_cores(self.dest, range(1, 10), online=False)
-
-
 class PowerManagementTests(PowerManagementTestsBase):
     """Test suite for a single host with 9 dedicated cores and 1 used for OS"""
 
@@ -267,42 +130,6 @@ class PowerManagementTests(PowerManagementTestsBase):
         unused_cpus = cpu_dedicated_set - instance_pcpus
         self._assert_cpu_set_state(unused_cpus, expected='offline')
 
-    def test_create_server_with_emulator_threads_isolate(self):
-        server = self._create_server(
-            flavor_id=self.isolate_flavor_id,
-            expected_state='ACTIVE')
-        # Let's verify that the pinned CPUs are now online
-        self._assert_server_cpus_state(server, expected='online')
-        instance = objects.Instance.get_by_uuid(self.ctxt, server['id'])
-        numa_topology = instance.numa_topology
-        # Make sure we've pinned the emulator threads to a separate core
-        self.assertTrue(numa_topology.cpuset_reserved)
-        self.assertTrue(
-            numa_topology.cpu_pinning.isdisjoint(
-                numa_topology.cpuset_reserved))
-        self._assert_cpu_set_state(numa_topology.cpuset_reserved,
-                                   expected='online')
-
-    def test_start_stop_server_with_emulator_threads_isolate(self):
-        server = self._create_server(
-            flavor_id=self.isolate_flavor_id,
-            expected_state='ACTIVE')
-        # Let's verify that the pinned CPUs are now online
-        self._assert_server_cpus_state(server, expected='online')
-        instance = objects.Instance.get_by_uuid(self.ctxt, server['id'])
-        numa_topology = instance.numa_topology
-        # Make sure we've pinned the emulator threads to a separate core
-        self.assertTrue(numa_topology.cpuset_reserved)
-        self.assertTrue(
-            numa_topology.cpu_pinning.isdisjoint(
-                numa_topology.cpuset_reserved))
-        self._assert_cpu_set_state(numa_topology.cpuset_reserved,
-                                   expected='online')
-        # Stop and assert we've powered down the emulator threads core as well
-        server = self._stop_server(server)
-        self._assert_cpu_set_state(numa_topology.cpuset_reserved,
-                                   expected='offline')
-
     def test_stop_start_server(self):
         server = self._create_server(
             flavor_id=self.pcpu_flavor_id,
@@ -393,39 +220,6 @@ class PowerManagementTestsGovernor(PowerManagementTestsBase):
                           self.restart_compute_service, hostname='compute1')
 
 
-class PowerManagementTestsGovernorNotSupported(PowerManagementTestsBase):
-    """Test suite for OS without governor support usage (same 10-core host)"""
-
-    def setUp(self):
-        super(PowerManagementTestsGovernorNotSupported, self).setUp()
-
-        self.useFixture(nova_fixtures.SysFileSystemFixture(
-            cpufreq_enabled=False))
-
-        # Definining the CPUs to be pinned.
-        self.flags(cpu_dedicated_set='1-9', cpu_shared_set=None,
-                   group='compute')
-        self.flags(vcpu_pin_set=None)
-        self.flags(cpu_power_management=True, group='libvirt')
-        self.flags(cpu_power_management_strategy='cpu_state', group='libvirt')
-
-        self.flags(allow_resize_to_same_host=True)
-        self.host_info = fakelibvirt.HostInfo(cpu_nodes=1, cpu_sockets=1,
-                                              cpu_cores=5, cpu_threads=2)
-
-    def test_enabling_governor_strategy_fails(self):
-        self.flags(cpu_power_management_strategy='governor', group='libvirt')
-        self.assertRaises(exception.FileNotFound, self.start_compute,
-                          host_info=self.host_info, hostname='compute1')
-
-    def test_enabling_cpu_state_strategy_works(self):
-        self.flags(cpu_power_management_strategy='cpu_state', group='libvirt')
-        self.compute1 = self.start_compute(host_info=self.host_info,
-                                           hostname='compute1')
-        cpu_dedicated_set = hardware.get_cpu_dedicated_set()
-        self._assert_cpu_set_state(cpu_dedicated_set, expected='offline')
-
-
 class PowerManagementMixedInstances(PowerManagementTestsBase):
     """Test suite for a single host with 6 dedicated cores, 3 shared and one
     OS-restricted.
diff --git a/nova/tests/functional/regressions/test_bug_2025480.py b/nova/tests/functional/regressions/test_bug_2025480.py
deleted file mode 100644
index c707a40a84..0000000000
--- a/nova/tests/functional/regressions/test_bug_2025480.py
+++ /dev/null
@@ -1,86 +0,0 @@
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-from unittest import mock
-
-from nova import context
-from nova.objects import compute_node
-from nova import test
-from nova.tests import fixtures as nova_fixtures
-from nova.tests.functional import fixtures as func_fixtures
-from nova.tests.functional import integrated_helpers
-
-
-class UnshelveUpdateAvailableResourcesPeriodicRace(
-        test.TestCase, integrated_helpers.InstanceHelperMixin):
-    def setUp(self):
-        super(UnshelveUpdateAvailableResourcesPeriodicRace, self).setUp()
-
-        placement = func_fixtures.PlacementFixture()
-        self.useFixture(placement)
-        self.placement = placement.api
-        self.neutron = nova_fixtures.NeutronFixture(self)
-        self.useFixture(self.neutron)
-        self.useFixture(nova_fixtures.GlanceFixture(self))
-        # Start nova services.
-        self.api = self.useFixture(nova_fixtures.OSAPIFixture(
-            api_version='v2.1')).admin_api
-        self.api.microversion = 'latest'
-        self.notifier = self.useFixture(
-            nova_fixtures.NotificationFixture(self))
-
-        self.start_service('conductor')
-        self.start_service('scheduler')
-
-    def test_unshelve_spawning_update_available_resources(self):
-        compute = self._start_compute('compute1')
-
-        server = self._create_server(
-            networks=[{'port': self.neutron.port_1['id']}])
-
-        node = compute_node.ComputeNode.get_by_nodename(
-            context.get_admin_context(), 'compute1')
-        self.assertEqual(1, node.vcpus_used)
-
-        # with default config shelve means immediate offload as well
-        req = {
-            'shelve': {}
-        }
-        self.api.post_server_action(server['id'], req)
-        self._wait_for_server_parameter(
-            server, {'status': 'SHELVED_OFFLOADED',
-                     'OS-EXT-SRV-ATTR:host': None})
-
-        node = compute_node.ComputeNode.get_by_nodename(
-            context.get_admin_context(), 'compute1')
-        self.assertEqual(0, node.vcpus_used)
-
-        def fake_spawn(*args, **kwargs):
-            self._run_periodics()
-
-        with mock.patch.object(
-                compute.driver, 'spawn', side_effect=fake_spawn):
-            req = {'unshelve': None}
-            self.api.post_server_action(server['id'], req)
-            self.notifier.wait_for_versioned_notifications(
-                'instance.unshelve.start')
-            self._wait_for_server_parameter(
-                server,
-                {
-                    'status': 'ACTIVE',
-                    'OS-EXT-STS:task_state': None,
-                    'OS-EXT-SRV-ATTR:host': 'compute1',
-                })
-
-        node = compute_node.ComputeNode.get_by_nodename(
-            context.get_admin_context(), 'compute1')
-        # After the fix, the instance should have resources claimed
-        self.assertEqual(1, node.vcpus_used)
diff --git a/nova/tests/functional/test_boot_from_volume.py b/nova/tests/functional/test_boot_from_volume.py
index a95167542e..6396954bf4 100644
--- a/nova/tests/functional/test_boot_from_volume.py
+++ b/nova/tests/functional/test_boot_from_volume.py
@@ -200,22 +200,6 @@ class BootFromVolumeTest(integrated_helpers._IntegratedTestBase):
         self.assertIn('You specified more local devices than the limit allows',
                       str(ex))
 
-    def test_reboot_bfv_instance(self):
-        # verify bdm 'source_type': 'image' and 'destination_type': 'volume'
-        server = self._create_server_boot_from_volume()
-        server = self._reboot_server(server, hard=True)
-        self.assertEqual(server['status'], 'ACTIVE')
-
-    def test_reboot_bfv_instance_snapshot(self):
-        # verify bdm 'source_type': 'snapshot' and 'destination_type': 'volume'
-        server = self._create_server_boot_from_volume()
-        self._snapshot_server(server, 'snap1')
-        images = self.api.get_images()
-        snap_img = [img for img in images if img['name'] == 'snap1'][0]
-        server1 = self._create_server(image_uuid=snap_img['id'])
-        self._reboot_server(server1, hard=True)
-        self.assertEqual(server['status'], 'ACTIVE')
-
 
 class BootFromVolumeLargeRequestTest(test.TestCase,
                                      integrated_helpers.InstanceHelperMixin):
diff --git a/nova/tests/unit/compute/test_stats.py b/nova/tests/unit/compute/test_stats.py
index b95475f09d..e713794a19 100644
--- a/nova/tests/unit/compute/test_stats.py
+++ b/nova/tests/unit/compute/test_stats.py
@@ -208,22 +208,6 @@ class StatsTestCase(test.NoDBTestCase):
         self.assertEqual(0, self.stats.num_os_type("Linux"))
         self.assertEqual(0, self.stats["num_vm_" + vm_states.BUILDING])
 
-    def test_update_stats_for_instance_being_unshelved(self):
-        instance = self._create_instance()
-        self.stats.update_stats_for_instance(instance)
-        self.assertEqual(1, self.stats.num_instances_for_project("1234"))
-
-        instance["vm_state"] = vm_states.SHELVED_OFFLOADED
-        instance["task_state"] = task_states.SPAWNING
-        self.stats.update_stats_for_instance(instance)
-
-        self.assertEqual(1, self.stats.num_instances)
-        self.assertEqual(1, self.stats.num_instances_for_project(1234))
-        self.assertEqual(1, self.stats["num_os_type_Linux"])
-        self.assertEqual(1, self.stats["num_vm_%s" %
-                                       vm_states.SHELVED_OFFLOADED])
-        self.assertEqual(1, self.stats["num_task_%s" % task_states.SPAWNING])
-
     def test_io_workload(self):
         vms = [vm_states.ACTIVE, vm_states.BUILDING, vm_states.PAUSED]
         tasks = [task_states.RESIZE_MIGRATING, task_states.REBUILDING,
diff --git a/nova/tests/unit/network/test_neutron.py b/nova/tests/unit/network/test_neutron.py
index a9aa501a13..0789022cfa 100644
--- a/nova/tests/unit/network/test_neutron.py
+++ b/nova/tests/unit/network/test_neutron.py
@@ -3615,8 +3615,8 @@ class TestAPI(TestAPIBase):
 
         subnets = self.api._get_subnets_from_port(self.context, port_data)
 
-        self.assertEqual(subnet_data1[0]['enable_dhcp'],
-                         subnets[0]['meta']['enable_dhcp'])
+        self.assertEqual(subnet_data1[0]['gateway_ip'],
+                         subnets[0]['meta']['dhcp_server'])
 
     @mock.patch.object(neutronapi, 'get_client', return_value=mock.Mock())
     def test_get_physnet_tunneled_info_multi_segment(self, mock_get_client):
diff --git a/nova/tests/unit/objects/test_instance_numa.py b/nova/tests/unit/objects/test_instance_numa.py
index 5a4c6da1cf..0d3bd0dba0 100644
--- a/nova/tests/unit/objects/test_instance_numa.py
+++ b/nova/tests/unit/objects/test_instance_numa.py
@@ -29,12 +29,10 @@ fake_obj_numa_topology = objects.InstanceNUMATopology(
     instance_uuid=fake_instance_uuid,
     cells=[
         objects.InstanceNUMACell(
-            id=0, cpuset=set(), pcpuset=set([1, 2]),
-            cpuset_reserved=set([5, 6]), memory=512,
+            id=0, cpuset=set(), pcpuset=set([1, 2]), memory=512,
             pagesize=2048),
         objects.InstanceNUMACell(
-            id=1, cpuset=set(), pcpuset=set([3, 4]),
-            cpuset_reserved=set([7, 8]), memory=512,
+            id=1, cpuset=set(), pcpuset=set([3, 4]), memory=512,
             pagesize=2048),
     ])
 
@@ -157,10 +155,6 @@ class _TestInstanceNUMACell(object):
         topo_obj.cells[1].pin_vcpus((3, 0), (4, 1))
         self.assertEqual(set([0, 1, 10, 11]), topo_obj.cpu_pinning)
 
-    def test_cpuset_reserved(self):
-        topo_obj = get_fake_obj_numa_topology(self.context)
-        self.assertEqual(set([5, 6, 7, 8]), topo_obj.cpuset_reserved)
-
     def test_clear_host_pinning(self):
         topo_obj = get_fake_obj_numa_topology(self.context)
         topo_obj.cells[0].pin_vcpus((1, 10), (2, 11))
diff --git a/nova/tests/unit/virt/libvirt/cpu/test_api.py b/nova/tests/unit/virt/libvirt/cpu/test_api.py
index 408496dbd3..18b927302f 100644
--- a/nova/tests/unit/virt/libvirt/cpu/test_api.py
+++ b/nova/tests/unit/virt/libvirt/cpu/test_api.py
@@ -24,7 +24,6 @@ class TestAPI(test.NoDBTestCase):
     def setUp(self):
         super(TestAPI, self).setUp()
         self.core_1 = api.Core(1)
-        self.api = api.API()
 
         # Create a fake instance with two pinned CPUs but only one is on the
         # dedicated set
@@ -58,12 +57,6 @@ class TestAPI(test.NoDBTestCase):
         self.assertEqual('fake_governor', self.core_1.governor)
         mock_get_governor.assert_called_once_with(self.core_1.ident)
 
-    @mock.patch.object(core, 'get_governor')
-    def test_governor_optional(self, mock_get_governor):
-        mock_get_governor.side_effect = exception.FileNotFound(file_path='foo')
-        self.assertIsNone(self.core_1.governor)
-        mock_get_governor.assert_called_once_with(self.core_1.ident)
-
     @mock.patch.object(core, 'set_governor')
     def test_set_governor_low(self, mock_set_governor):
         self.flags(cpu_power_governor_low='fake_low_gov', group='libvirt')
@@ -83,7 +76,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management=True, group='libvirt')
         self.flags(cpu_dedicated_set='1-2', group='compute')
 
-        self.api.power_up_for_instance(self.fake_inst)
+        api.power_up(self.fake_inst)
         # only core #2 can be set as core #0 is not on the dedicated set
         # As a reminder, core(i).online calls set_online(i)
         mock_online.assert_called_once_with(2)
@@ -94,7 +87,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management_strategy='governor', group='libvirt')
         self.flags(cpu_dedicated_set='1-2', group='compute')
 
-        self.api.power_up_for_instance(self.fake_inst)
+        api.power_up(self.fake_inst)
         # only core #2 can be set as core #1 is not on the dedicated set
         # As a reminder, core(i).set_high_governor calls set_governor(i)
         mock_set_governor.assert_called_once_with(2, 'performance')
@@ -102,13 +95,13 @@ class TestAPI(test.NoDBTestCase):
     @mock.patch.object(core, 'set_online')
     def test_power_up_skipped(self, mock_online):
         self.flags(cpu_power_management=False, group='libvirt')
-        self.api.power_up_for_instance(self.fake_inst)
+        api.power_up(self.fake_inst)
         mock_online.assert_not_called()
 
     @mock.patch.object(core, 'set_online')
     def test_power_up_skipped_if_standard_instance(self, mock_online):
         self.flags(cpu_power_management=True, group='libvirt')
-        self.api.power_up_for_instance(objects.Instance(numa_topology=None))
+        api.power_up(objects.Instance(numa_topology=None))
         mock_online.assert_not_called()
 
     @mock.patch.object(core, 'set_offline')
@@ -116,7 +109,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management=True, group='libvirt')
         self.flags(cpu_dedicated_set='1-2', group='compute')
 
-        self.api.power_down_for_instance(self.fake_inst)
+        api.power_down(self.fake_inst)
         # only core #2 can be set as core #1 is not on the dedicated set
         # As a reminder, core(i).online calls set_online(i)
         mock_offline.assert_called_once_with(2)
@@ -127,7 +120,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management_strategy='governor', group='libvirt')
         self.flags(cpu_dedicated_set='0-1', group='compute')
 
-        self.api.power_down_for_instance(self.fake_inst)
+        api.power_down(self.fake_inst)
 
         # Make sure that core #0 is ignored, since it is special and cannot
         # be powered down.
@@ -139,7 +132,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management_strategy='governor', group='libvirt')
         self.flags(cpu_dedicated_set='1-2', group='compute')
 
-        self.api.power_down_for_instance(self.fake_inst)
+        api.power_down(self.fake_inst)
 
         # only core #2 can be set as core #0 is not on the dedicated set
         # As a reminder, core(i).set_high_governor calls set_governor(i)
@@ -148,13 +141,13 @@ class TestAPI(test.NoDBTestCase):
     @mock.patch.object(core, 'set_offline')
     def test_power_down_skipped(self, mock_offline):
         self.flags(cpu_power_management=False, group='libvirt')
-        self.api.power_down_for_instance(self.fake_inst)
+        api.power_down(self.fake_inst)
         mock_offline.assert_not_called()
 
     @mock.patch.object(core, 'set_offline')
     def test_power_down_skipped_if_standard_instance(self, mock_offline):
         self.flags(cpu_power_management=True, group='libvirt')
-        self.api.power_down_for_instance(objects.Instance(numa_topology=None))
+        api.power_down(objects.Instance(numa_topology=None))
         mock_offline.assert_not_called()
 
     @mock.patch.object(core, 'set_offline')
@@ -162,7 +155,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management=True, group='libvirt')
         self.flags(cpu_dedicated_set='0-2', group='compute')
 
-        self.api.power_down_all_dedicated_cpus()
+        api.power_down_all_dedicated_cpus()
         # All dedicated CPUs are turned offline, except CPU0
         mock_offline.assert_has_calls([mock.call(1), mock.call(2)])
 
@@ -172,7 +165,7 @@ class TestAPI(test.NoDBTestCase):
         self.flags(cpu_power_management_strategy='governor', group='libvirt')
         self.flags(cpu_dedicated_set='0-2', group='compute')
 
-        self.api.power_down_all_dedicated_cpus()
+        api.power_down_all_dedicated_cpus()
         # All dedicated CPUs are turned offline, except CPU0
         mock_set_governor.assert_has_calls([mock.call(1, 'powersave'),
                                             mock.call(2, 'powersave')])
@@ -180,7 +173,7 @@ class TestAPI(test.NoDBTestCase):
     @mock.patch.object(core, 'set_offline')
     def test_power_down_all_dedicated_cpus_skipped(self, mock_offline):
         self.flags(cpu_power_management=False, group='libvirt')
-        self.api.power_down_all_dedicated_cpus()
+        api.power_down_all_dedicated_cpus()
         mock_offline.assert_not_called()
 
     @mock.patch.object(core, 'set_offline')
@@ -189,7 +182,7 @@ class TestAPI(test.NoDBTestCase):
     ):
         self.flags(cpu_power_management=True, group='libvirt')
         self.flags(cpu_dedicated_set=None, group='compute')
-        self.api.power_down_all_dedicated_cpus()
+        api.power_down_all_dedicated_cpus()
         mock_offline.assert_not_called()
 
     @mock.patch.object(core, 'get_governor')
@@ -202,7 +195,7 @@ class TestAPI(test.NoDBTestCase):
         mock_get_governor.return_value = 'performance'
         mock_get_online.side_effect = (True, False)
         self.assertRaises(exception.InvalidConfiguration,
-                          self.api.validate_all_dedicated_cpus)
+                          api.validate_all_dedicated_cpus)
 
     @mock.patch.object(core, 'get_governor')
     @mock.patch.object(core, 'get_online')
@@ -214,7 +207,7 @@ class TestAPI(test.NoDBTestCase):
         mock_get_online.return_value = True
         mock_get_governor.side_effect = ('powersave', 'performance')
         self.assertRaises(exception.InvalidConfiguration,
-                          self.api.validate_all_dedicated_cpus)
+                          api.validate_all_dedicated_cpus)
 
     @mock.patch.object(core, 'get_governor')
     @mock.patch.object(core, 'get_online')
@@ -228,7 +221,7 @@ class TestAPI(test.NoDBTestCase):
         mock_get_online.return_value = True
         mock_get_governor.return_value = 'performance'
 
-        self.api.validate_all_dedicated_cpus()
+        api.validate_all_dedicated_cpus()
 
         # Make sure we skipped CPU0
         mock_get_online.assert_has_calls([mock.call(1), mock.call(2)])
@@ -241,6 +234,45 @@ class TestAPI(test.NoDBTestCase):
     def test_validate_all_dedicated_cpus_no_cpu(self):
         self.flags(cpu_power_management=True, group='libvirt')
         self.flags(cpu_dedicated_set=None, group='compute')
-        self.api.validate_all_dedicated_cpus()
+        api.validate_all_dedicated_cpus()
         # no assert we want to make sure the validation won't raise if
         # no dedicated cpus are configured
+
+    @mock.patch.object(core, 'get_governor')
+    @mock.patch.object(core, 'get_online')
+    def test_validate_all_dedicated_cpus_for_cpu_state_no_governor_ignored(
+        self, mock_get_online, mock_get_governor
+    ):
+        self.flags(cpu_power_management=True, group='libvirt')
+        self.flags(cpu_dedicated_set='0-2', group='compute')
+        self.flags(cpu_power_management_strategy='cpu_state', group='libvirt')
+
+        mock_get_online.return_value = True
+        mock_get_governor.side_effect = FileNotFoundError(
+            "File /sys/devices/system/cpu/cpu1/cpufreq/scaling_governor "
+            "could not be found.")
+
+        api.validate_all_dedicated_cpus()
+
+        self.assertEqual(2, len(mock_get_governor.mock_calls))
+
+    @mock.patch.object(core, 'get_governor')
+    @mock.patch.object(core, 'get_online')
+    def test_validate_all_dedicated_cpus_for_governor_error(
+        self, mock_get_online, mock_get_governor
+    ):
+        self.flags(cpu_power_management=True, group='libvirt')
+        self.flags(cpu_dedicated_set='0-2', group='compute')
+        self.flags(cpu_power_management_strategy='governor', group='libvirt')
+
+        mock_get_online.return_value = True
+        mock_get_governor.side_effect = FileNotFoundError(
+            "File /sys/devices/system/cpu/cpu1/cpufreq/scaling_governor "
+            "could not be found.")
+
+        ex = self.assertRaises(
+            exception.InvalidConfiguration, api.validate_all_dedicated_cpus)
+        self.assertIn(
+            "[libvirt]cpu_power_management_strategy is 'governor', "
+            "but the host OS does not support governors for CPU0",
+            str(ex))
diff --git a/nova/tests/unit/virt/libvirt/test_driver.py b/nova/tests/unit/virt/libvirt/test_driver.py
index 4ae3b7a7ab..a28df59ecb 100644
--- a/nova/tests/unit/virt/libvirt/test_driver.py
+++ b/nova/tests/unit/virt/libvirt/test_driver.py
@@ -27934,15 +27934,10 @@ class LibvirtDriverTestCase(test.NoDBTestCase, TraitsComparisonMixin):
         self.assertTrue(hv.synic)
         self.assertTrue(hv.reset)
         self.assertTrue(hv.frequencies)
-        # NOTE(jie) reenlightenment will cause instances live-migration
-        # failure, so don't enable it now. See bug 2046549.
-        self.assertFalse(hv.reenlightenment)
+        self.assertTrue(hv.reenlightenment)
         self.assertTrue(hv.tlbflush)
         self.assertTrue(hv.ipi)
-        # NOTE(artom) evmcs only works on Intel hosts, so we can't enable it
-        # unconditionally. Until we become smarter about it, just don't enable
-        # it at all. See bug 2009280.
-        self.assertFalse(hv.evmcs)
+        self.assertTrue(hv.evmcs)
 
 
 class LibvirtVolumeUsageTestCase(test.NoDBTestCase):
diff --git a/nova/virt/hardware.py b/nova/virt/hardware.py
index a8733796ef..afb0742e4a 100644
--- a/nova/virt/hardware.py
+++ b/nova/virt/hardware.py
@@ -76,6 +76,36 @@ def get_cpu_dedicated_set():
     return cpu_ids
 
 
+def get_cpu_stable_set():
+    """Parse ``[compute] cpu_stable_set`` config.
+
+    :returns: A set of host CPU IDs that can be used for prioritized PCPU allocations.
+    """
+    if not CONF.compute.cpu_stable_set:
+        return None
+
+    cpu_ids = parse_cpu_spec(CONF.compute.cpu_stable_set)
+    if not cpu_ids:
+        msg = _("No CPUs available after parsing '[compute] "
+                "cpu_stable_set' config, %r")
+        raise exception.Invalid(msg % CONF.compute.cpu_stable_set)
+    return cpu_ids
+
+def get_cpu_dynamic_set():
+    """Parse ``[compute] cpu_dynamic_set`` config.
+
+    :returns: A set of host CPU IDs that can be used for prioritized PCPU allocations.
+    """
+    if not CONF.compute.cpu_dynamic_set:
+        return None
+
+    cpu_ids = parse_cpu_spec(CONF.compute.cpu_dynamic_set)
+    if not cpu_ids:
+        msg = _("No CPUs available after parsing '[compute] "
+                "cpu_dynamic_set' config, %r")
+        raise exception.Invalid(msg % CONF.compute.cpu_dynamic_set)
+    return cpu_ids
+
 def get_cpu_dedicated_set_nozero():
     """Return cpu_dedicated_set without CPU0, if present"""
     return (get_cpu_dedicated_set() or set()) - {0}
@@ -720,8 +750,28 @@ def _pack_instance_onto_cores(host_cell, instance_cell,
         #
         # For an instance_cores=[2, 3], usable_cores=[[0], [4]]
         # vcpus_pinning=[(2, 0), (3, 4)]
-        vcpus_pinning = list(zip(sorted(instance_cores),
-                                 itertools.chain(*usable_cores)))
+
+        def get_priority_weight(val, high_p_list):
+            if val in high_p_list:
+                return 0
+            return 1
+
+        cpu_stable_set = get_cpu_stable_set()
+        usable_cores_list = list(itertools.chain(*usable_cores))
+        if len(cpu_stable_set) > 1:
+            usable_cores_list = sorted(usable_cores_list, key=lambda x: get_priority_weight(x, cpu_stable_set))
+            msg = ("Using priority core pinning: high priority cores: "
+                   "%(cpu_stable_set)s, priority ordered host cores: %(usable_cores_list)s")
+            msg_args = {
+                'cpu_stable_set': cpu_stable_set,
+                'usable_cores_list': usable_cores_list,
+            }
+            LOG.info(msg, msg_args)
+        vcpus_pinning = list(zip(
+            sorted(instance_cores),
+            usable_cores_list
+        ))
+
         msg = ("Computed NUMA topology CPU pinning: usable pCPUs: "
                "%(usable_cores)s, vCPUs mapping: %(vcpus_pinning)s")
         msg_args = {
diff --git a/nova/virt/libvirt/cpu/api.py b/nova/virt/libvirt/cpu/api.py
index 1c4bd19bee..0673b9e26f 100644
--- a/nova/virt/libvirt/cpu/api.py
+++ b/nova/virt/libvirt/cpu/api.py
@@ -11,7 +11,6 @@
 #    under the License.
 
 from dataclasses import dataclass
-import typing as ty
 
 from oslo_log import log as logging
 
@@ -60,13 +59,8 @@ class Core:
         return str(self.ident)
 
     @property
-    def governor(self) -> ty.Optional[str]:
-        try:
-            return core.get_governor(self.ident)
-        # NOTE(sbauza): cpufreq/scaling_governor is not enabled for some OS
-        # platforms.
-        except exception.FileNotFound:
-            return None
+    def governor(self) -> str:
+        return core.get_governor(self.ident)
 
     def set_high_governor(self) -> None:
         core.set_governor(self.ident, CONF.libvirt.cpu_power_governor_high)
@@ -75,126 +69,103 @@ class Core:
         core.set_governor(self.ident, CONF.libvirt.cpu_power_governor_low)
 
 
-class API(object):
-
-    def core(self, i):
-        """From a purely functional point of view, there is no need for this
-        method. However, we want to test power management in multinode
-        scenarios (ex: live migration) in our functional tests. If we
-        instantiated the Core class directly in the methods below, the
-        functional tests would not be able to distinguish between cores on the
-        source and destination hosts. In functional tests we can replace this
-        helper method by a stub that returns a fixture, allowing us to maintain
-        distinct core power state for each host.
-
-        See also nova.virt.libvirt.driver.LibvirtDriver.cpu_api.
-        """
-        return Core(i)
-
-    def _power_up(self, cpus: ty.Set[int]) -> None:
-        if not CONF.libvirt.cpu_power_management:
-            return
-        cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
-        powered_up = set()
-        for cpu in cpus:
-            if cpu in cpu_dedicated_set:
-                pcpu = self.core(cpu)
-                if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
-                    pcpu.online = True
-                else:
-                    pcpu.set_high_governor()
-                powered_up.add(str(pcpu))
-        LOG.debug("Cores powered up : %s", powered_up)
-
-    def power_up_for_instance(self, instance: objects.Instance) -> None:
-        if instance.numa_topology is None:
-            return
-        pcpus = instance.numa_topology.cpu_pinning.union(
-            instance.numa_topology.cpuset_reserved)
-        self._power_up(pcpus)
-
-    def power_up_for_migration(
-        self, dst_numa_info: objects.LibvirtLiveMigrateNUMAInfo
-    ) -> None:
-        pcpus = set()
-        if 'emulator_pins' in dst_numa_info and dst_numa_info.emulator_pins:
-            pcpus = dst_numa_info.emulator_pins
-        for pins in dst_numa_info.cpu_pins.values():
-            pcpus = pcpus.union(pins)
-        self._power_up(pcpus)
-
-    def _power_down(self, cpus: ty.Set[int]) -> None:
-        if not CONF.libvirt.cpu_power_management:
-            return
-        cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
-        powered_down = set()
-        for cpu in cpus:
-            if cpu in cpu_dedicated_set:
-                pcpu = self.core(cpu)
-                if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
-                    pcpu.online = False
-                else:
-                    pcpu.set_low_governor()
-                powered_down.add(str(pcpu))
-        LOG.debug("Cores powered down : %s", powered_down)
-
-    def power_down_for_migration(
-        self, dst_numa_info: objects.LibvirtLiveMigrateNUMAInfo
-    ) -> None:
-        pcpus = set()
-        if 'emulator_pins' in dst_numa_info and dst_numa_info.emulator_pins:
-            pcpus = dst_numa_info.emulator_pins
-        for pins in dst_numa_info.cpu_pins.values():
-            pcpus = pcpus.union(pins)
-        self._power_down(pcpus)
-
-    def power_down_for_instance(self, instance: objects.Instance) -> None:
-        if instance.numa_topology is None:
-            return
-        pcpus = instance.numa_topology.cpu_pinning.union(
-            instance.numa_topology.cpuset_reserved)
-        self._power_down(pcpus)
-
-    def power_down_all_dedicated_cpus(self) -> None:
-        if not CONF.libvirt.cpu_power_management:
-            return
-
-        cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
-        for pcpu in cpu_dedicated_set:
-            pcpu = self.core(pcpu)
+def power_up(instance: objects.Instance) -> None:
+    if not CONF.libvirt.cpu_power_management:
+        return
+    if instance.numa_topology is None:
+        return
+
+    cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
+    pcpus = instance.numa_topology.cpu_pinning
+    powered_up = set()
+    for pcpu in pcpus:
+        if pcpu in cpu_dedicated_set:
+            pcpu = Core(pcpu)
+            if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
+                pcpu.online = True
+            else:
+                pcpu.set_high_governor()
+            powered_up.add(str(pcpu))
+    LOG.debug("Cores powered up : %s", powered_up)
+
+
+def power_down(instance: objects.Instance) -> None:
+    if not CONF.libvirt.cpu_power_management:
+        return
+    if instance.numa_topology is None:
+        return
+
+    cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
+    pcpus = instance.numa_topology.cpu_pinning
+    powered_down = set()
+    for pcpu in pcpus:
+        if pcpu in cpu_dedicated_set:
+            pcpu = Core(pcpu)
             if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
                 pcpu.online = False
             else:
                 pcpu.set_low_governor()
-        LOG.debug("Cores powered down : %s", cpu_dedicated_set)
-
-    def validate_all_dedicated_cpus(self) -> None:
-        if not CONF.libvirt.cpu_power_management:
-            return
-        cpu_dedicated_set = hardware.get_cpu_dedicated_set() or set()
-        governors = set()
-        cpu_states = set()
-        for pcpu in cpu_dedicated_set:
-            if (pcpu == 0 and
-                    CONF.libvirt.cpu_power_management_strategy == 'cpu_state'):
-                LOG.warning('CPU0 is in cpu_dedicated_set, '
-                            'but it is not eligible for state management '
-                            'and will be ignored')
-                continue
-            pcpu = self.core(pcpu)
-            # we need to collect the governors strategy and the CPU states
-            governors.add(pcpu.governor)
-            cpu_states.add(pcpu.online)
+            powered_down.add(str(pcpu))
+    LOG.debug("Cores powered down : %s", powered_down)
+
+
+def power_down_all_dedicated_cpus() -> None:
+    if not CONF.libvirt.cpu_power_management:
+        return
+
+    cpu_dedicated_set = hardware.get_cpu_dedicated_set_nozero() or set()
+    for pcpu in cpu_dedicated_set:
+        pcpu = Core(pcpu)
         if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
-            # all the cores need to have the same governor strategy
-            if len(governors) > 1:
-                msg = _("All the cores need to have the same governor strategy"
-                        "before modifying the CPU states. You can reboot the "
-                        "compute node if you prefer.")
-                raise exception.InvalidConfiguration(msg)
-        elif CONF.libvirt.cpu_power_management_strategy == 'governor':
-            # all the cores need to be online
-            if False in cpu_states:
-                msg = _("All the cores need to be online before modifying the "
-                        "governor strategy.")
-                raise exception.InvalidConfiguration(msg)
+            pcpu.online = False
+        else:
+            pcpu.set_low_governor()
+    LOG.debug("Cores powered down : %s", cpu_dedicated_set)
+
+
+def validate_all_dedicated_cpus() -> None:
+    if not CONF.libvirt.cpu_power_management:
+        return
+    cpu_dedicated_set = hardware.get_cpu_dedicated_set() or set()
+    governors = set()
+    cpu_states = set()
+    for pcpu in cpu_dedicated_set:
+        if (pcpu == 0 and
+                CONF.libvirt.cpu_power_management_strategy == 'cpu_state'):
+            LOG.warning('CPU0 is in cpu_dedicated_set, but it is not eligible '
+                        'for state management and will be ignored')
+            continue
+        pcpu = Core(pcpu)
+        # we need to collect the governors strategy and the CPU states
+        try:
+            governors.add(pcpu.governor)
+        except FileNotFoundError as e:
+            # NOTE(gibi): When
+            # /sys/devices/system/cpu/cpuX/cpufreq/scaling_governor does
+            # not exist it means the host OS does not support any governors.
+            # If cpu_state strategy is requested we can ignore this as
+            # governors will not be used but if governor strategy is requested
+            # we need to report an error and stop as the host is not properly
+            # configured
+            if CONF.libvirt.cpu_power_management_strategy == 'governor':
+                msg = _(
+                    "[libvirt]cpu_power_management_strategy is 'governor', "
+                    "but the host OS does not support governors for CPU%d"
+                    % pcpu.ident
+                )
+                raise exception.InvalidConfiguration(msg) from e
+
+        cpu_states.add(pcpu.online)
+    if CONF.libvirt.cpu_power_management_strategy == 'cpu_state':
+        # all the cores need to have the same governor strategy
+        if len(governors) > 1:
+            msg = _("All the cores need to have the same governor strategy"
+                    "before modifying the CPU states. You can reboot the "
+                    "compute node if you prefer.")
+            raise exception.InvalidConfiguration(msg)
+    elif CONF.libvirt.cpu_power_management_strategy == 'governor':
+        # all the cores need to be online
+        if False in cpu_states:
+            msg = _("All the cores need to be online before modifying the "
+                    "governor strategy.")
+            raise exception.InvalidConfiguration(msg)
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 994d88f16c..804aef22b4 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -544,15 +544,6 @@ class LibvirtDriver(driver.ComputeDriver):
         # events about success or failure.
         self._device_event_handler = AsyncDeviceEventsHandler()
 
-        # NOTE(artom) From a pure functionality point of view, there's no need
-        # for this to be an attribute of self. However, we want to test power
-        # management in multinode scenarios (ex: live migration) in our
-        # functional tests. If the power management code was just a bunch of
-        # module level functions, the functional tests would not be able to
-        # distinguish between cores on the source and destination hosts.
-        # See also nova.virt.libvirt.cpu.api.API.core().
-        self.cpu_api = libvirt_cpu.API()
-
     def _discover_vpmems(self, vpmem_conf=None):
         """Discover vpmems on host and configuration.
 
@@ -833,13 +824,13 @@ class LibvirtDriver(driver.ComputeDriver):
         # modified by Nova before. Note that it can provide an exception if
         # either the governor strategies are different between the cores or if
         # the cores are offline.
-        self.cpu_api.validate_all_dedicated_cpus()
+        libvirt_cpu.validate_all_dedicated_cpus()
         # NOTE(sbauza): We powerdown all dedicated CPUs but if some instances
         # exist that are pinned for some CPUs, then we'll later powerup those
         # CPUs when rebooting the instance in _init_instance()
         # Note that it can provide an exception if the config options are
         # wrongly modified.
-        self.cpu_api.power_down_all_dedicated_cpus()
+        libvirt_cpu.power_down_all_dedicated_cpus()
 
         # TODO(sbauza): Remove this code once mediated devices are persisted
         # across reboots.
@@ -1563,7 +1554,7 @@ class LibvirtDriver(driver.ComputeDriver):
             if CONF.libvirt.virt_type == 'lxc':
                 self._teardown_container(instance)
             # We're sure the instance is gone, we can shutdown the core if so
-            self.cpu_api.power_down_for_instance(instance)
+            libvirt_cpu.power_down(instance)
 
     def destroy(self, context, instance, network_info, block_device_info=None,
                 destroy_disks=True, destroy_secrets=True):
@@ -3217,7 +3208,7 @@ class LibvirtDriver(driver.ComputeDriver):
 
         current_power_state = guest.get_power_state(self._host)
 
-        self.cpu_api.power_up_for_instance(instance)
+        libvirt_cpu.power_up(instance)
         # TODO(stephenfin): Any reason we couldn't use 'self.resume' here?
         guest.launch(pause=current_power_state == power_state.PAUSED)
 
@@ -6236,8 +6227,10 @@ class LibvirtDriver(driver.ComputeDriver):
             hv.synic = True
             hv.reset = True
             hv.frequencies = True
+            hv.reenlightenment = True
             hv.tlbflush = True
             hv.ipi = True
+            hv.evmcs = True
 
             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
             # driver to work on windows VMs. At the moment, the nvidia driver
@@ -7705,7 +7698,7 @@ class LibvirtDriver(driver.ComputeDriver):
                 post_xml_callback()
 
             if power_on or pause:
-                self.cpu_api.power_up_for_instance(instance)
+                libvirt_cpu.power_up(instance)
                 guest.launch(pause=pause)
 
             return guest
@@ -10783,16 +10776,6 @@ class LibvirtDriver(driver.ComputeDriver):
                         serial_console.release_port(
                             host=migrate_data.serial_listen_addr, port=port)
 
-                if (
-                    'dst_numa_info' in migrate_data and
-                    migrate_data.dst_numa_info
-                ):
-                    self.cpu_api.power_down_for_migration(
-                        migrate_data.dst_numa_info)
-                else:
-                    LOG.debug('No dst_numa_info in migrate_data, '
-                              'no cores to power down in rollback.')
-
             if not is_shared_instance_path:
                 instance_dir = libvirt_utils.get_instance_path_at_destination(
                     instance, migrate_data)
@@ -10959,12 +10942,6 @@ class LibvirtDriver(driver.ComputeDriver):
 
                 migrate_data.bdms.append(bdmi)
 
-        if 'dst_numa_info' in migrate_data and migrate_data.dst_numa_info:
-            self.cpu_api.power_up_for_migration(migrate_data.dst_numa_info)
-        else:
-            LOG.debug('No dst_numa_info in migrate_data, '
-                      'no cores to power up in pre_live_migration.')
-
         return migrate_data
 
     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
@@ -11128,7 +11105,6 @@ class LibvirtDriver(driver.ComputeDriver):
         :param network_info: instance network information
         """
         self.unplug_vifs(instance, network_info)
-        self.cpu_api.power_down_for_instance(instance)
 
     def _qemu_monitor_announce_self(self, instance):
         """Send announce_self command to QEMU monitor.
diff --git a/nova/virt/libvirt/host.py b/nova/virt/libvirt/host.py
index b57751093e..6e296ce23f 100644
--- a/nova/virt/libvirt/host.py
+++ b/nova/virt/libvirt/host.py
@@ -38,6 +38,7 @@ import queue
 import socket
 import threading
 import typing as ty
+import requests as rq
 
 from eventlet import greenio
 from eventlet import greenthread
@@ -60,7 +61,7 @@ from nova.objects import fields
 from nova.pci import utils as pci_utils
 from nova import rpc
 from nova import utils
-from nova.virt import event as virtevent
+from nova.virt import event as virtevent, hardware
 from nova.virt.libvirt import config as vconfig
 from nova.virt.libvirt import event as libvirtevent
 from nova.virt.libvirt import guest as libvirt_guest
@@ -760,7 +761,29 @@ class Host(object):
             if cpu_map[cpu]:
                 online_cpus.add(cpu)
 
-        return online_cpus
+        asleep_cpus = self._get_sleeping_cpus()
+        os_online_cpus = online_cpus - asleep_cpus
+        if len(asleep_cpus) > 0:
+            LOG.info('Asleep cpus detected. %(libvirsh_online_cpus)s:%(asleep_cpus)s for '
+                     'os_online %(os_online_cpus)s',
+                     {'libvirsh_online_cpus': online_cpus, 'asleep_cpus': asleep_cpus,
+                      'os_online_cpus': os_online_cpus})
+
+        return os_online_cpus
+
+    def _get_sleeping_cpus(self):
+        """Get the ids of the cores that are sleeping.
+
+        :returns: list of ids that are asleep.
+        """
+        endpoint = CONF.compute.cpu_sleep_info_endpoint
+        r = rq.get(url=endpoint)
+        data = r.json()
+        is_awake = data['is-awake']
+        sleeping_cpus = set()
+        if not is_awake:
+            sleeping_cpus = set(hardware.get_cpu_dynamic_set())
+        return sleeping_cpus
 
     def get_cpu_model_names(self):
         """Get the cpu models based on host CPU arch
diff --git a/nova/virt/netutils.py b/nova/virt/netutils.py
index 2bc78134a1..0ab3ddc4c1 100644
--- a/nova/virt/netutils.py
+++ b/nova/virt/netutils.py
@@ -289,8 +289,7 @@ def _get_nets(vif, subnet, version, net_num, link_id):
     net_type = ''
     if subnet.get_meta('ipv6_address_mode') is not None:
         net_type = '_%s' % subnet.get_meta('ipv6_address_mode')
-    elif (subnet.get_meta('dhcp_server') is not None or
-          subnet.get_meta('enable_dhcp')):
+    elif subnet.get_meta('dhcp_server') is not None:
         net_info = {
             'id': 'network%d' % net_num,
             'type': 'ipv%d_dhcp' % version,
diff --git a/releasenotes/notes/bug-2023018-0f93ca1f679ce259.yaml b/releasenotes/notes/bug-2023018-0f93ca1f679ce259.yaml
deleted file mode 100644
index 7d6dfc43db..0000000000
--- a/releasenotes/notes/bug-2023018-0f93ca1f679ce259.yaml
+++ /dev/null
@@ -1,9 +0,0 @@
----
-fixes:
-  - |
-    Some OS platforms don't provide by default cpufreq resources in sysfs, so
-    they don't have CPU scaling governors. That's why we should let the governor
-    strategy to be optional for `CPU power management`_.
-
-    .. _CPU power management: https://docs.openstack.org/nova/latest/admin/cpu-topologies.html#configuring-cpu-power-management-for-dedicated-cores
-
diff --git a/releasenotes/notes/bug-2055245-fix-nova-metadata-api-f60b18b1b594bec0.yaml b/releasenotes/notes/bug-2055245-fix-nova-metadata-api-f60b18b1b594bec0.yaml
deleted file mode 100644
index 9db969926a..0000000000
--- a/releasenotes/notes/bug-2055245-fix-nova-metadata-api-f60b18b1b594bec0.yaml
+++ /dev/null
@@ -1,9 +0,0 @@
----
-fixes:
-  - |
-    With the change from ml2/ovs DHCP agents towards OVN implementation
-    in neutron there is no port with device_owner ``network:dhcp`` anymore.
-    Instead DHCP is provided by ``network:distributed`` port.
-    Fix relies on enable_dhcp provided by neutron-api if no port with
-    ``network:dhcp`` owner is found. See `bug 2055245
-    <https://bugs.launchpad.net/nova/+bug/2055245>`__ for details.
diff --git a/releasenotes/notes/libvirt-enlightenments-stop-unconditionally-enabling-evmcs-993a825641c4b9f3.yaml b/releasenotes/notes/libvirt-enlightenments-stop-unconditionally-enabling-evmcs-993a825641c4b9f3.yaml
deleted file mode 100644
index 31609f2a2d..0000000000
--- a/releasenotes/notes/libvirt-enlightenments-stop-unconditionally-enabling-evmcs-993a825641c4b9f3.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
----
-fixes:
-  - |
-    Bug 2009280 has been fixed by no longer enabling the evmcs enlightenment in
-    the libvirt driver. evmcs only works on Intel CPUs, and domains with that
-    enlightenment cannot be started on AMD hosts. There is a possible future
-    feature to enable support for generating this enlightenment only when
-    running on Intel hosts.
